# backend/app/services/token_tracking.py
# Token kullanÄ±m takip sistemi

from typing import List, Dict, Any, Tuple


class TokenTracker:
    """Her LLM Ã§aÄŸrÄ±sÄ±nÄ±n token kullanÄ±mÄ±nÄ± izler ve toplar."""
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_tokens = 0
        self.call_details: List[Dict[str, Any]] = []
        
    def add_usage(self, input_tokens: int, output_tokens: int, step_name: str, 
                  estimated: bool = False, raw_metadata: Dict[str, Any] = None):
        """Token kullanÄ±mÄ±nÄ± ekle."""
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
        self.total_tokens = self.total_input_tokens + self.total_output_tokens
        
        detail = {
            "step": step_name,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total": input_tokens + output_tokens,
            "estimated": estimated,
            "metadata": raw_metadata
        }
        self.call_details.append(detail)
        
        print(f"\nğŸ“Š TOKEN KULLANIMI ({step_name}):")
        print(f"   GiriÅŸ: {input_tokens:,} | Ã‡Ä±kÄ±ÅŸ: {output_tokens:,} | Toplam: {input_tokens + output_tokens:,}")
        if estimated:
            print(f"   âš ï¸ Bu deÄŸerler tahminidir (metadata'dan alÄ±namadÄ±)")
        print(f"   ğŸ“ˆ TOPLAM: GiriÅŸ={self.total_input_tokens:,} | Ã‡Ä±kÄ±ÅŸ={self.total_output_tokens:,} | Toplam={self.total_tokens:,}\n")
    
    def get_summary(self) -> Dict[str, Any]:
        """Token kullanÄ±m Ã¶zetini dÃ¶ndÃ¼r."""
        # Gemini 1.5 Flash / 2.0 Flash FiyatlandÄ±rmasÄ± (YaklaÅŸÄ±k):
        # Girdi (input): $0.10 per 1M tokens
        # Ã‡Ä±ktÄ± (output): $0.40 per 1M tokens
        # NOT: Fiyatlar deÄŸiÅŸebilir, Google Cloud Pricing sayfasÄ±nÄ± kontrol edin.
        input_cost_per_million = 0.10
        output_cost_per_million = 0.40
        
        input_cost = (self.total_input_tokens / 1_000_000) * input_cost_per_million
        output_cost = (self.total_output_tokens / 1_000_000) * output_cost_per_million
        total_cost_usd = input_cost + output_cost
        
        return {
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_tokens": self.total_tokens,
            "call_count": len(self.call_details),
            "breakdown": self.call_details,
            "estimated_cost_usd": total_cost_usd,
            "estimated_cost_tl": total_cost_usd * 35  # ~35 TL/USD
        }


def estimate_tokens_from_text(text: str) -> int:
    """Metinden token sayÄ±sÄ±nÄ± tahmin et (Google Gemini iÃ§in ~4 karakter = 1 token)."""
    if not text:
        return 0
    # TÃ¼rkÃ§e karakterler daha az token kullanÄ±r, Ä°ngilizce daha fazla
    # Ortalama olarak ~3-4 karakter = 1 token alÄ±yoruz
    return max(1, len(text) // 3)


def extract_token_usage_from_response(response, step_name: str, prompt_text: str = None) -> Tuple[int, int]:
    """LangChain response'dan token bilgisini Ã§Ä±kar. BaÅŸarÄ±sÄ±z olursa tahmin yap."""
    input_tokens, output_tokens = 0, 0
    
    # Debug iÃ§in raw metadata'yÄ± yazdÄ±r
    if hasattr(response, 'response_metadata'):
        print(f"ğŸ” DEBUG ({step_name}) Raw Metadata: {response.response_metadata}")
    elif hasattr(response, 'usage_metadata'):
        print(f"ğŸ” DEBUG ({step_name}) Usage Metadata: {response.usage_metadata}")
    
    # 1. Ã–nce AIMessage'Ä±n direkt usage_metadata Ã¶zelliÄŸini kontrol et
    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        usage = response.usage_metadata
        if isinstance(usage, dict):
            input_tokens = usage.get('prompt_token_count', usage.get('input_tokens', 0))
            output_tokens = usage.get('candidates_token_count', usage.get('output_tokens', 0))
            # OpenAI formatÄ±
            if input_tokens == 0 and 'input_tokens' in usage:
                input_tokens = usage['input_tokens']
            if output_tokens == 0 and 'output_tokens' in usage:
                output_tokens = usage['output_tokens']
                
            if input_tokens > 0 or output_tokens > 0:
                print(f"âœ… ({step_name}): Token bilgisi 'usage_metadata' Ã¶zelliÄŸinden alÄ±ndÄ±.")
                return input_tokens, output_tokens
        elif hasattr(usage, 'prompt_token_count'):
            input_tokens = usage.prompt_token_count
            output_tokens = getattr(usage, 'candidates_token_count', getattr(usage, 'completion_token_count', 0))
            if input_tokens > 0 or output_tokens > 0:
                print(f"âœ… ({step_name}): Token bilgisi 'usage_metadata' nesnesinden alÄ±ndÄ±.")
                return input_tokens, output_tokens
    
    # 2. response_metadata iÃ§inde usage_metadata kontrolÃ¼
    if hasattr(response, 'response_metadata') and response.response_metadata:
        metadata = response.response_metadata
        
        # usage_metadata kontrolÃ¼ (Google Gemini formatÄ±)
        if 'usage_metadata' in metadata:
            gemini_usage = metadata.get('usage_metadata', {})
            # Ollama formatÄ± iÃ§in prompt_eval_count ve eval_count kontrolÃ¼
            if 'prompt_eval_count' in gemini_usage or 'eval_count' in gemini_usage:
                input_tokens = gemini_usage.get('prompt_eval_count', 0)
                output_tokens = gemini_usage.get('eval_count', 0)
                if input_tokens > 0 or output_tokens > 0:
                    print(f"âœ… ({step_name}): Token bilgisi 'response_metadata.usage_metadata' iÃ§inden alÄ±ndÄ± (Ollama formatÄ±).")
                    return input_tokens, output_tokens
            
            # Gemini formatÄ±
            input_tokens = gemini_usage.get('prompt_token_count', 0)
            output_tokens = gemini_usage.get('candidates_token_count', 0)
            
            # EÄŸer hala 0 ise, total_token_count'a bak (bazÄ± versiyonlarda sadece bu olabilir)
            if input_tokens == 0 and output_tokens == 0 and 'total_token_count' in gemini_usage:
                total = gemini_usage.get('total_token_count', 0)
                # Tahmini daÄŸÄ±lÄ±m yap (input genellikle daha Ã§oktur RAG'de)
                if total > 0:
                    print(f"âš ï¸ ({step_name}): Sadece toplam token var, tahmini daÄŸÄ±tÄ±lÄ±yor.")
                    input_tokens = int(total * 0.8)
                    output_tokens = total - input_tokens
            
            if input_tokens > 0 or output_tokens > 0:
                print(f"âœ… ({step_name}): Token bilgisi 'response_metadata.usage_metadata' iÃ§inden alÄ±ndÄ±.")
                return input_tokens, output_tokens
        
        # token_usage kontrolÃ¼ (genel format / OpenAI)
        if 'token_usage' in metadata:
            usage = metadata.get('token_usage', {})
            input_tokens = usage.get('prompt_tokens', usage.get('input_tokens', 0))
            output_tokens = usage.get('completion_tokens', usage.get('output_tokens', 0))
            if input_tokens > 0 or output_tokens > 0:
                print(f"âœ… ({step_name}): Token bilgisi 'response_metadata.token_usage' iÃ§inden alÄ±ndÄ±.")
                return input_tokens, output_tokens
                
        # Anthropic formatÄ± (usage)
        if 'usage' in metadata:
            usage = metadata.get('usage', {})
            input_tokens = usage.get('input_tokens', 0)
            output_tokens = usage.get('output_tokens', 0)
            if input_tokens > 0 or output_tokens > 0:
                print(f"âœ… ({step_name}): Token bilgisi 'response_metadata.usage' iÃ§inden alÄ±ndÄ± (Anthropic).")
                return input_tokens, output_tokens
    
    # 3. EÄŸer metadata'dan alÄ±namadÄ±ysa, metin uzunluÄŸuna gÃ¶re tahmin yap
    # Output token tahmini (response content'ten)
    if hasattr(response, 'content') and response.content:
        estimated_output = estimate_tokens_from_text(response.content)
    else:
        estimated_output = 0
    
    # Input token tahmini (prompt'tan)
    if prompt_text:
        estimated_input = estimate_tokens_from_text(prompt_text)
    else:
        # Prompt bilgisi yoksa, response'dan geriye doÄŸru tahmin yap
        # Genellikle input, output'un 5-10 katÄ± olabilir (uzun promptlar iÃ§in)
        estimated_input = estimated_output * 5 if estimated_output > 0 else 0
    
    # EÄŸer hiÃ§ token yoksa, en azÄ±ndan kÃ¼Ã§Ã¼k bir deÄŸer ver
    if estimated_input == 0 and estimated_output == 0:
        print(f"âš ï¸ UYARI ({step_name}): Token bilgisi hiÃ§bir ÅŸekilde alÄ±namadÄ±!")
        return 0, 0
    
    print(f"âš ï¸ UYARI ({step_name}): Metadata'dan token bilgisi alÄ±namadÄ±, tahmin kullanÄ±lÄ±yor.")
    print(f"   Tahmin: GiriÅŸ={estimated_input}, Ã‡Ä±kÄ±ÅŸ={estimated_output}")
    return estimated_input, estimated_output

