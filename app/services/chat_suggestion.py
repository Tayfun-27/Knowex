# backend/app/services/chat_suggestion.py
import logging
from collections import defaultdict
from typing import Dict, Optional

from app.schemas.chat import ChatRequest, ChatResponse, ActiveContextFile
from app.schemas.user import UserInDB
from app.repositories.base import BaseRepository
from app.services import vector_service
from app.services import rag_ranking  # _calculate_filename_match_score iÃ§in
from app.services import rag_retrievers # create_hypothetical_document iÃ§in
from app.services.chat_context import ContextMemory
from app.services import chat_context # <-- YENÄ° IMPORT
from app.core import config as app_config

logger = logging.getLogger(__name__)

# --- Config-backed thresholds ---
FILENAME_MATCH_STRONG = getattr(app_config, 'FILENAME_MATCH_STRONG', 1.9)
VECTOR_SCORE_MIN = getattr(app_config, 'VECTOR_SCORE_MIN', 0.75)
CHAMPION_MIN_CHUNKS = getattr(app_config, 'CHAMPION_MIN_CHUNKS', 4)

def handle_suggestion_flow(
    request: ChatRequest, 
    user: UserInDB, 
    db: BaseRepository, 
    context_memory: ContextMemory,
    chat_id: str
) -> Optional[ChatResponse]:
    """
    BaÄŸlam yoksa (is_general_search) bir dosya Ã¶nermeye Ã§alÄ±ÅŸÄ±r.
    Ã–neri bulunursa ChatResponse dÃ¶ndÃ¼rÃ¼r, bulunamazsa None dÃ¶ndÃ¼rÃ¼r.
    """
    if context_memory.has_context():
        return None  # Zaten baÄŸlam var, Ã¶neri akÄ±ÅŸÄ±nÄ± atla

    logger.info("BaÄŸlam yok. 'Ã–neri' iÃ§in GeliÅŸtirilmiÅŸ HÄ±zlÄ± Arama yapÄ±lÄ±yor...")
    best_match_file = None
    
    # --- DEÄÄ°ÅÄ°KLÄ°K 1: TÃ¼m dosyalar yerine sadece izin verilenleri al ---
    # ESKÄ°: all_files = db.get_all_files_for_tenant(user.tenant_id)
    all_files = chat_context.get_all_accessible_files_for_user(db, user)
    # --- DEÄÄ°ÅÄ°KLÄ°K BÄ°TTÄ° ---

    highest_score = FILENAME_MATCH_STRONG
    
    # 1. Dosya AdÄ± EÅŸleÅŸmesi
    for file in all_files:
        score = rag_ranking._calculate_filename_match_score(request.message, file.name)
        if score > highest_score:
            highest_score = score
            best_match_file = file
    
    if best_match_file:
        logger.info(f"ğŸ’¡ GÃ¼Ã§lÃ¼ aday (Dosya AdÄ± EÅŸleÅŸmesi) bulundu: {best_match_file.name}")
    else:
        # 2. VektÃ¶r EÅŸleÅŸmesi
        logger.info("Dosya adÄ±yla tam eÅŸleÅŸme yok. 'Ã–neri' iÃ§in YETKÄ° FÄ°LTRELÄ° vektÃ¶r aramasÄ± yapÄ±lÄ±yor...")
        
        # --- DEÄÄ°ÅÄ°KLÄ°K 2: VektÃ¶r aramasÄ±nÄ± izin verilen dosya ID'leri ile kÄ±sÄ±tla ---
        allowed_file_ids_for_search = {file.id for file in all_files}
        if not allowed_file_ids_for_search:
            logger.info("KullanÄ±cÄ±nÄ±n eriÅŸebileceÄŸi dosya yok, Ã¶neri bulunamadÄ±.")
            return None # Arama yapacak dosyasÄ± yok
        # --- DEÄÄ°ÅÄ°KLÄ°K BÄ°TTÄ° ---

        hyde_query = rag_retrievers.create_hypothetical_document(request.message, None)
        quick_search_chunks = vector_service.search_similar_chunks(
            tenant_id=user.tenant_id, 
            query=hyde_query, 
            db=db, 
            limit=50, 
            filter_file_ids=allowed_file_ids_for_search # <-- GÃœNCELLENDÄ°
        )
        
        if quick_search_chunks:
            file_scores: Dict[str, float] = defaultdict(float)
            file_counts: Dict[str, int] = defaultdict(int)
            for chunk in quick_search_chunks:
                file_id = chunk.get("source_file_id")
                score = chunk.get("similarity_score", 0.0)
                if file_id and score > VECTOR_SCORE_MIN:
                    file_scores[file_id] += score
                    file_counts[file_id] += 1
            
            if not file_scores:
                 logger.info("AlakalÄ± vektÃ¶r sonucu bulunamadÄ± (skor eÅŸiÄŸi).")
            else:
                # ... (Bu kÄ±sÄ±m (ÅŸampiyon belirleme) olduÄŸu gibi kalabilir) ...
                sorted_by_score = sorted(file_scores.items(), key=lambda item: item[1], reverse=True)
                champion_file_id, champion_total_score = sorted_by_score[0]
                champion_chunk_count = file_counts[champion_file_id]
                is_dominant_by_score = False
                
                if len(sorted_by_score) > 1:
                    second_best_score = sorted_by_score[1][1]
                    if champion_total_score > (second_best_score * 1.5):
                        is_dominant_by_score = True
                else:
                    is_dominant_by_score = True
                    
                if champion_chunk_count >= CHAMPION_MIN_CHUNKS and is_dominant_by_score:
                    logger.info(f"ğŸ’¡ GÃ¼Ã§lÃ¼ aday (VektÃ¶r) bulundu: {champion_file_id} (Chunk SayÄ±sÄ±: {champion_chunk_count}, Toplam Skor: {champion_total_score:.2f})")
                    best_match_file = db.get_file_by_id(user.tenant_id, champion_file_id)
                else:
                     logger.info(f"ZayÄ±f aday: {champion_file_id} (Chunk: {champion_chunk_count}, Skor: {champion_total_score:.2f}). Dominant deÄŸil veya chunk sayÄ±sÄ± yetersiz.")
    
    # ... (Ã–neri dÃ¶ndÃ¼rme kÄ±smÄ± olduÄŸu gibi kalabilir) ...
    if best_match_file:
        suggested_file_context = ActiveContextFile(id=best_match_file.id, name=best_match_file.name, type="file")
        response_msg = f"Sorunuzun '{best_match_file.name}' dosyasÄ±yla ilgili olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼yorum. Bu dosyayÄ± baÄŸlama ekleyerek devam edeyim mi?"
        return ChatResponse(
            response_message=response_msg,
            source_context=None,
            chat_id=chat_id,
            active_context_files=context_memory.get_context(),
            response_type="suggestion",
            suggested_file=suggested_file_context
        )
    
    logger.info("Otomatik Ã¶neri iÃ§in gÃ¼Ã§lÃ¼ aday bulunamadÄ±. Genel RAG aramasÄ± yapÄ±lacak.")
    return None