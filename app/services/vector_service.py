# backend/app/services/vector_service.py - OPTÄ°MÄ°ZE EDÄ°LDÄ° (Lazy Loading)
# LocalGPT tarzÄ± Hibrit Arama ve Reranking desteÄŸi eklendi

import re
from typing import List, Set, Optional, Dict, Tuple
from functools import lru_cache  # <-- Ã–NEMLÄ°: Bu eklendi
from collections import defaultdict

from app.schemas.file import FileOut
from app.schemas.user import UserInDB
from app.repositories.base import BaseRepository
from app.storage_adapters.base import BaseStorageAdapter
from app.core import parsers, chunker
from app.core.config import GEMINI_API_KEY

# --- LangChain ImportlarÄ± ---
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_firestore import FirestoreVectorStore
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever

# --- BM25 iÃ§in ---
try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    print("âš ï¸ rank-bm25 paketi yÃ¼klÃ¼ deÄŸil. BM25 aramasÄ± devre dÄ±ÅŸÄ±.")

# --- Firestore FieldFilter ---
from google.cloud.firestore_v1.base_query import FieldFilter

# --- KRÄ°TÄ°K DEÄÄ°ÅÄ°KLÄ°K: Global deÄŸiÅŸkeni SÄ°LDÄ°K ---
# embedding_model = GoogleGenerativeAIEmbeddings(...) # BU SATIR ARTIK YOK

# --- YERÄ°NE BU FONKSÄ°YONU EKLEDÄ°K ---
@lru_cache(maxsize=1)
def get_embedding_model():
    """
    Embedding modelini uygulama aÃ§Ä±lÄ±rken deÄŸil,
    Ä°LK KEZ Ä°HTÄ°YAÃ‡ DUYULDUÄUNDA yÃ¼kler.
    """
    print("âš¡ Embedding modeli ilk kez yÃ¼kleniyor (Lazy Load)...")
    try:
        return GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004", 
            google_api_key=GEMINI_API_KEY
        )
    except Exception as e:
        print(f"Google Generative AI Embeddings yapÄ±landÄ±rÄ±lamadÄ±: {e}")
        return None

def index_file(
    file_record: FileOut,
    user: UserInDB,
    db: BaseRepository,
    storage: BaseStorageAdapter
):
    """DosyayÄ± vektÃ¶rleÅŸtirip kaydeder."""
    print(f"Indexing iÅŸlemi baÅŸladÄ±: {file_record.name}")

    try:
        # DEÄÄ°ÅÄ°KLÄ°K: Global deÄŸiÅŸken yerine fonksiyonu Ã§aÄŸÄ±rÄ±yoruz
        embedding_model = get_embedding_model() # <-- BURASI DEÄÄ°ÅTÄ°

        if not embedding_model:
            raise Exception("Embedding modeli baÅŸlatÄ±lamadÄ±ÄŸÄ± iÃ§in indexleme yapÄ±lamÄ±yor.")

        # EÄŸer dosya external storage'dan geliyorsa, Google Drive/OneDrive'dan geÃ§ici olarak indir
        if file_record.external_file_id and file_record.external_storage_type:
            from google.cloud import firestore
            from app.storage_adapters.google_drive_adapter import GoogleDriveAdapter
            from app.storage_adapters.onedrive_adapter import OneDriveAdapter
            from app.core.config import GOOGLE_DRIVE_CLIENT_ID, GOOGLE_DRIVE_CLIENT_SECRET, ONEDRIVE_CLIENT_ID, ONEDRIVE_CLIENT_SECRET
            
            firestore_db = firestore.Client()
            storage_type = file_record.external_storage_type
            
            # KullanÄ±cÄ±nÄ±n storage baÄŸlantÄ±sÄ±nÄ± al
            if storage_type == "google_drive":
                user_storage = firestore_db.collection("user_external_storage").document(user.id).get()
                if not user_storage.exists:
                    # Admin seviyesinde baÄŸlantÄ±yÄ± kontrol et
                    admin_settings = firestore_db.collection("external_storage_settings").document(user.tenant_id).get()
                    if not admin_settings.exists:
                        print(f"âš ï¸ Google Drive baÄŸlantÄ±sÄ± bulunamadÄ±, indexleme atlanÄ±yor: {file_record.name}")
                        return
                    admin_data = admin_settings.to_dict()
                    access_token = admin_data.get('google_drive_access_token')
                    refresh_token = admin_data.get('google_drive_refresh_token')
                    client_id = GOOGLE_DRIVE_CLIENT_ID
                    client_secret = GOOGLE_DRIVE_CLIENT_SECRET
                else:
                    storage_data = user_storage.to_dict()
                    access_token = storage_data.get('access_token')
                    refresh_token = storage_data.get('refresh_token')
                    client_id = GOOGLE_DRIVE_CLIENT_ID
                    client_secret = GOOGLE_DRIVE_CLIENT_SECRET
                
                adapter = GoogleDriveAdapter()
            elif storage_type == "onedrive":
                user_storage = firestore_db.collection("user_external_storage").document(user.id).get()
                if not user_storage.exists:
                    # Admin seviyesinde baÄŸlantÄ±yÄ± kontrol et
                    admin_settings = firestore_db.collection("external_storage_settings").document(user.tenant_id).get()
                    if not admin_settings.exists:
                        print(f"âš ï¸ OneDrive baÄŸlantÄ±sÄ± bulunamadÄ±, indexleme atlanÄ±yor: {file_record.name}")
                        return
                    admin_data = admin_settings.to_dict()
                    access_token = admin_data.get('onedrive_access_token')
                    refresh_token = admin_data.get('onedrive_refresh_token')
                    client_id = ONEDRIVE_CLIENT_ID
                    client_secret = ONEDRIVE_CLIENT_SECRET
                else:
                    storage_data = user_storage.to_dict()
                    access_token = storage_data.get('access_token')
                    refresh_token = storage_data.get('refresh_token')
                    client_id = ONEDRIVE_CLIENT_ID
                    client_secret = ONEDRIVE_CLIENT_SECRET
                
                adapter = OneDriveAdapter()
            else:
                print(f"âš ï¸ Desteklenmeyen storage tipi: {storage_type}, indexleme atlanÄ±yor: {file_record.name}")
                return
            
            if not access_token:
                print(f"âš ï¸ Access token bulunamadÄ±, indexleme atlanÄ±yor: {file_record.name}")
                return
            
            # Token'Ä± kontrol et ve gerekirse yenile
            try:
                if storage_type == "google_drive":
                    file_bytes = adapter.download_file(
                        file_id=file_record.external_file_id,
                        access_token=access_token,
                        mime_type=file_record.content_type
                    )
                else:  # onedrive
                    file_bytes = adapter.download_file(
                        file_id=file_record.external_file_id,
                        access_token=access_token
                    )
            except Exception as e:
                # Token sÃ¼resi dolmuÅŸ olabilir
                if refresh_token and client_id and client_secret:
                    try:
                        tokens = adapter.refresh_access_token(
                            refresh_token=refresh_token,
                            client_id=client_id,
                            client_secret=client_secret
                        )
                        access_token = tokens['access_token']
                        
                        # Token'Ä± gÃ¼ncelle
                        if user_storage.exists:
                            firestore_db.collection("user_external_storage").document(user.id).update({
                                'access_token': access_token
                            })
                        else:
                            # Admin seviyesinde gÃ¼ncelle
                            update_data = {}
                            if storage_type == "google_drive":
                                update_data['google_drive_access_token'] = access_token
                            else:
                                update_data['onedrive_access_token'] = access_token
                            firestore_db.collection("external_storage_settings").document(user.tenant_id).update(update_data)
                        
                        # Tekrar dene
                        if storage_type == "google_drive":
                            file_bytes = adapter.download_file(
                                file_id=file_record.external_file_id,
                                access_token=access_token,
                                mime_type=file_record.content_type
                            )
                        else:
                            file_bytes = adapter.download_file(
                                file_id=file_record.external_file_id,
                                access_token=access_token
                            )
                    except Exception as refresh_error:
                        print(f"âš ï¸ Token yenileme baÅŸarÄ±sÄ±z, indexleme atlanÄ±yor ({file_record.name}): {refresh_error}")
                        return
                else:
                    print(f"âš ï¸ Dosya indirilemedi, indexleme atlanÄ±yor ({file_record.name}): {e}")
                    return
        else:
            # Normal dosyalar iÃ§in mevcut mantÄ±k
            if not file_record.storage_path:
                print(f"âš ï¸ Storage path bulunamadÄ±, indexleme atlanÄ±yor: {file_record.name}")
                return
            file_bytes = storage.download_file_content(storage_path=file_record.storage_path)
        
        text_content = parsers.extract_text_from_file(
            file_bytes=file_bytes,
            file_name=file_record.name,
            mime_type=file_record.content_type
        )

        if not text_content or text_content.strip().startswith("["):
            print(f"Ä°Ã§erik okunamadÄ±: {file_record.name}")
            return

        metadata_prefix = ""
        match = re.search(r"(?:To|AlÄ±cÄ±)\s*:\s*(.*)", text_content, re.IGNORECASE)
        if match:
            customer_name = match.group(1).strip().split('\n')[0].strip()
            if customer_name:
                metadata_prefix = f"Bu dokÃ¼man, '{customer_name}' mÃ¼ÅŸterisine aittir.\n"

        chunks = chunker.get_text_chunks(text_content, chunk_size=1000, overlap=150)
        batch_size = 5
        chunk_batch = []

        for i, chunk_text in enumerate(chunks):
            enriched_chunk_text = metadata_prefix + chunk_text
            # Embedding oluÅŸtur
            embedding = embedding_model.embed_documents([enriched_chunk_text])[0]

            if embedding:
                chunk_data = {
                    "tenant_id": user.tenant_id,
                    "file_id": file_record.id,
                    "file_name": file_record.name,
                    "chunk_number": i,
                    "chunk_text": enriched_chunk_text,
                    "embedding": embedding
                }
                chunk_batch.append(chunk_data)

            if len(chunk_batch) >= batch_size:
                db.add_text_chunks_batch(chunk_batch)
                chunk_batch = []

        if chunk_batch:
            db.add_text_chunks_batch(chunk_batch)

        print(f"Indexing tamamlandÄ±: {file_record.name}")

    except Exception as e:
        print(f"Dosya indexlenirken hata: {e}")


def search_similar_chunks(
    tenant_id: str,
    query: str,
    db: BaseRepository,
    limit: int = 15,
    filter_file_ids: Optional[Set[str]] = None
) -> List[dict]:
    """KullanÄ±cÄ± sorusuna benzer metinleri arar."""
    try:
        # DEÄÄ°ÅÄ°KLÄ°K: Global deÄŸiÅŸken yerine fonksiyonu Ã§aÄŸÄ±rÄ±yoruz
        embedding_model = get_embedding_model() # <-- BURASI DEÄÄ°ÅTÄ°
        
        if not embedding_model:
            raise Exception("Embedding modeli baÅŸlatÄ±lamadÄ±.")
            
        query_embedding = embedding_model.embed_query(query)
        
        if not query_embedding:
            raise Exception("Sorgu vektÃ¶rÃ¼ oluÅŸturulamadÄ±.")

        similar_chunks = db.find_similar_chunks(
            tenant_id=tenant_id,
            query_vector=query_embedding,
            limit=limit,
            file_id_filter=filter_file_ids
        )

        results = [
            {
                "id": chunk.get("id"),
                "text": chunk.get("chunk_text"),
                "source_file_name": chunk.get("file_name"),
                "source_file_id": chunk.get("file_id"),
                "similarity_score": chunk.get("similarity_score", 0.0)
            }
            for chunk in similar_chunks
        ]
        return results

    except Exception as e:
        print(f"VektÃ¶r aramasÄ± hatasÄ±: {e}")
        return []


# --- BM25 Index Cache (tenant bazlÄ±) ---
_bm25_index_cache: Dict[str, Tuple] = {}  # {cache_key: (bm25_index, chunk_list)}


def _tokenize_turkish(text: str) -> List[str]:
    """TÃ¼rkÃ§e metni tokenize et (basit yaklaÅŸÄ±m)."""
    # TÃ¼rkÃ§e karakterleri koru, kÃ¼Ã§Ã¼k harfe Ã§evir, kelimelere bÃ¶l
    text = text.lower()
    # Noktalama iÅŸaretlerini kaldÄ±r, kelimelere bÃ¶l
    words = re.findall(r'\b\w+\b', text)
    return words


def build_bm25_index(
    tenant_id: str,
    db: BaseRepository,
    filter_file_ids: Optional[Set[str]] = None
) -> Tuple:
    """BM25 index oluÅŸtur (cache'lenir)."""
    if not BM25_AVAILABLE:
        return None, []
    
    # Cache key oluÅŸtur
    filter_key = hash(tuple(sorted(filter_file_ids or []))) if filter_file_ids else None
    cache_key = f"{tenant_id}_{filter_key}"
    
    if cache_key in _bm25_index_cache:
        print(f"ğŸ“¦ BM25 index cache'den yÃ¼klendi (tenant: {tenant_id})")
        return _bm25_index_cache[cache_key]
    
    print(f"ğŸ”¨ BM25 index oluÅŸturuluyor (tenant: {tenant_id})...")
    
    # TÃ¼m chunk'larÄ± al
    all_chunks = []
    try:
        # Firestore'dan chunk'larÄ± Ã§ek
        if hasattr(db, 'db'):
            chunks_collection = db.db.collection("text_chunks")
            query = chunks_collection.where(filter=FieldFilter("tenant_id", "==", tenant_id))
            
            if filter_file_ids and len(filter_file_ids) <= 10:
                # Firestore 'in' operatÃ¶rÃ¼ limiti 10
                file_id_list = list(filter_file_ids)[:10]
                query = query.where(filter=FieldFilter("file_id", "in", file_id_list))
            elif filter_file_ids and len(filter_file_ids) > 10:
                # Ã‡ok fazla file_id varsa, tÃ¼m chunk'larÄ± al ve sonra filtrele
                print(f"âš ï¸ {len(filter_file_ids)} file_id var, tÃ¼m chunk'lar alÄ±nÄ±p sonra filtrelenecek")
            
            docs = query.stream()
            
            for doc in docs:
                doc_data = doc.to_dict()
                chunk_file_id = doc_data.get("file_id", "")
                
                # EÄŸer filter_file_ids varsa ve 10'dan fazlaysa, manuel filtrele
                if filter_file_ids and len(filter_file_ids) > 10:
                    if chunk_file_id not in filter_file_ids:
                        continue
                
                all_chunks.append({
                    "id": doc.id,
                    "text": doc_data.get("chunk_text", ""),
                    "file_id": chunk_file_id,
                    "file_name": doc_data.get("file_name", ""),
                })
        else:
            print("âš ï¸ Firestore repository'ye eriÅŸilemedi, BM25 index oluÅŸturulamadÄ±")
            return None, []
            
    except Exception as e:
        print(f"âš ï¸ BM25 index oluÅŸturulurken hata: {e}")
        return None, []
    
    if not all_chunks:
        print("âš ï¸ BM25 index iÃ§in chunk bulunamadÄ±")
        return None, []
    
    # Tokenize et
    tokenized_corpus = [_tokenize_turkish(chunk["text"]) for chunk in all_chunks]
    
    # BM25 index oluÅŸtur
    try:
        bm25 = BM25Okapi(tokenized_corpus)
    except Exception as e:
        print(f"âš ï¸ BM25 index oluÅŸturulamadÄ±: {e}")
        return None, []
    
    # Cache'e kaydet
    _bm25_index_cache[cache_key] = (bm25, all_chunks)
    
    print(f"âœ… BM25 index oluÅŸturuldu: {len(all_chunks)} chunk")
    return bm25, all_chunks


def search_with_bm25(
    tenant_id: str,
    query: str,
    db: BaseRepository,
    limit: int = 15,
    filter_file_ids: Optional[Set[str]] = None
) -> List[dict]:
    """BM25 tabanlÄ± keyword search yapar (LocalGPT tarzÄ±)."""
    if not BM25_AVAILABLE:
        return []
    
    try:
        # BM25 index oluÅŸtur veya cache'den al
        bm25_index, chunk_list = build_bm25_index(tenant_id, db, filter_file_ids)
        
        if not bm25_index or not chunk_list:
            return []
        
        # Query'yi tokenize et
        tokenized_query = _tokenize_turkish(query)
        
        if not tokenized_query:
            return []
        
        # BM25 skorlarÄ±nÄ± hesapla
        scores = bm25_index.get_scores(tokenized_query)
        
        # Skorlu chunk'larÄ± oluÅŸtur
        scored_chunks = []
        for i, (chunk, score) in enumerate(zip(chunk_list, scores)):
            if score > 0:  # Sadece pozitif skorlu chunk'larÄ± al
                scored_chunks.append({
                    "id": chunk["id"],
                    "text": chunk["text"],
                    "source_file_name": chunk.get("file_name", ""),
                    "source_file_id": chunk.get("file_id", ""),
                    "bm25_score": float(score),
                    "rank": i + 1
                })
        
        # Skora gÃ¶re sÄ±rala
        scored_chunks.sort(key=lambda x: x["bm25_score"], reverse=True)
        
        print(f"ğŸ” BM25 aramasÄ±: {len(scored_chunks)} sonuÃ§ bulundu (query: '{query[:50]}...')")
        
        return scored_chunks[:limit]
        
    except Exception as e:
        print(f"âŒ BM25 aramasÄ± hatasÄ±: {e}")
        return []


def hybrid_search_similar_chunks(
    tenant_id: str,
    query: str,
    db: BaseRepository,
    limit: int = 15,
    filter_file_ids: Optional[Set[str]] = None,
    retrieval_mode: str = "hybrid"  # "hybrid", "vector", "bm25"
) -> List[dict]:
    """
    LocalGPT tarzÄ± hibrit arama: Vector + BM25 + RRF.
    
    Args:
        retrieval_mode: "hybrid" (vector + bm25), "vector" (sadece semantic), "bm25" (sadece keyword)
    """
    try:
        if retrieval_mode == "vector":
            # Sadece semantic search
            return search_similar_chunks(tenant_id, query, db, limit, filter_file_ids)
        
        elif retrieval_mode == "bm25":
            # Sadece keyword search
            return search_with_bm25(tenant_id, query, db, limit, filter_file_ids)
        
        else:  # hybrid
            # 1. Vector search (semantic)
            vector_results = search_similar_chunks(
                tenant_id=tenant_id,
                query=query,
                db=db,
                limit=limit * 2,  # Daha fazla sonuÃ§ al
                filter_file_ids=filter_file_ids
            )
            
            # 2. BM25 search (keyword) - eÄŸer mevcut deÄŸilse sadece vector kullan
            bm25_results = []
            if BM25_AVAILABLE:
                bm25_results = search_with_bm25(
                    tenant_id=tenant_id,
                    query=query,
                    db=db,
                    limit=limit * 2,
                    filter_file_ids=filter_file_ids
                )
            
            # EÄŸer BM25 sonuÃ§ yoksa, sadece vector sonuÃ§larÄ±nÄ± dÃ¶ndÃ¼r
            if not bm25_results:
                print(f"âš ï¸ BM25 sonuÃ§ bulunamadÄ±, sadece vector search kullanÄ±lÄ±yor")
                return vector_results[:limit]
            
            # 3. Reciprocal Rank Fusion (RRF) - LocalGPT'in kullandÄ±ÄŸÄ± yÃ¶ntem
            # RRF_score = sum(1 / (k + rank)) for each retrieval method
            k = 60  # LocalGPT'in kullandÄ±ÄŸÄ± deÄŸer
            
            combined_results = {}
            
            # Vector sonuÃ§larÄ±nÄ± ekle
            for rank, result in enumerate(vector_results, 1):
                chunk_id = result.get("id")
                if chunk_id not in combined_results:
                    combined_results[chunk_id] = {
                        **result,
                        "vector_rank": rank,
                        "vector_score": result.get("similarity_score", 0.0),
                        "bm25_rank": None,
                        "bm25_score": 0.0,
                        "rrf_score": 0.0
                    }
            
            # BM25 sonuÃ§larÄ±nÄ± ekle
            for rank, result in enumerate(bm25_results, 1):
                chunk_id = result.get("id")
                if chunk_id in combined_results:
                    combined_results[chunk_id]["bm25_rank"] = rank
                    combined_results[chunk_id]["bm25_score"] = result.get("bm25_score", 0.0)
                else:
                    combined_results[chunk_id] = {
                        **result,
                        "vector_rank": None,
                        "vector_score": 0.0,
                        "bm25_rank": rank,
                        "bm25_score": result.get("bm25_score", 0.0),
                        "rrf_score": 0.0
                    }
            
            # RRF skoru hesapla
            for chunk_id, result in combined_results.items():
                rrf_score = 0.0
                
                # Vector RRF
                if result.get("vector_rank"):
                    rrf_score += 1.0 / (k + result["vector_rank"])
                
                # BM25 RRF
                if result.get("bm25_rank"):
                    rrf_score += 1.0 / (k + result["bm25_rank"])
                
                result["rrf_score"] = rrf_score
                result["hybrid_score"] = rrf_score  # AynÄ± deÄŸer (uyumluluk iÃ§in)
            
            # RRF score'a gÃ¶re sÄ±rala
            final_results = sorted(
                combined_results.values(),
                key=lambda x: x.get("rrf_score", 0.0),
                reverse=True
            )
            
            print(f"ğŸ”€ Hibrit arama (RRF): {len(vector_results)} vector + {len(bm25_results)} BM25 = {len(final_results)} birleÅŸik sonuÃ§")
            
            return final_results[:limit]
            
    except Exception as e:
        print(f"âŒ Hibrit arama hatasÄ±: {e}")
        # Fallback: Sadece semantic search
        return search_similar_chunks(tenant_id, query, db, limit, filter_file_ids)


def get_firestore_retriever(
    tenant_id: str,
    filter_file_ids: Optional[List[str]] = None
) -> BaseRetriever:
    """LangChain retriever oluÅŸturur."""
    # DEÄÄ°ÅÄ°KLÄ°K: Global deÄŸiÅŸken yerine fonksiyonu Ã§aÄŸÄ±rÄ±yoruz
    embedding_model = get_embedding_model() # <-- BURASI DEÄÄ°ÅTÄ°

    if not embedding_model:
        raise Exception("Embedding modeli yÃ¼klenemedi.")

    vector_store = FirestoreVectorStore(
        collection="text_chunks",
        embedding_service=embedding_model,
    )

    search_kwargs = {'k': 150}
    if filter_file_ids:
         # print(f"Retriever filtrelendi...")
         pass

    return vector_store.as_retriever(search_kwargs=search_kwargs)
def warmup_model_in_background():
    """
    Bu fonksiyon sunucu aÃ§Ä±ldÄ±ÄŸÄ±nda arka planda Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r.
    AmacÄ±: get_embedding_model() fonksiyonunu bir kez Ã§alÄ±ÅŸtÄ±rÄ±p
    Ã¶nbelleÄŸe (cache) alÄ±nmasÄ±nÄ± saÄŸlamaktÄ±r.
    """
    try:
        print("ğŸ”¥ Arka plan model Ä±sÄ±tma iÅŸlemi baÅŸladÄ±...")
        # Modeli Ã§aÄŸÄ±rarak lru_cache'in dolmasÄ±nÄ± saÄŸlÄ±yoruz
        model = get_embedding_model()
        if model:
            print("âœ… Model arka planda baÅŸarÄ±yla yÃ¼klendi ve hazÄ±r!")
        else:
            print("âš ï¸ Model Ä±sÄ±tma sÄ±rasÄ±nda yÃ¼klenemedi.")
    except Exception as e:
        print(f"âš ï¸ Model Ä±sÄ±tma hatasÄ±: {e}")