# backend/app/services/llm_providers.py
# LLM modelleri ve wrapper'larÄ±

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from pydantic import Field
import requests

from app.core.config import GEMINI_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY, OLLAMA_API_BASE_URL


class OllamaLLMWrapper(BaseChatModel):
    """Ollama API'sini LangChain uyumlu hale getiren wrapper."""
    
    base_url: str = Field(description="Ollama API base URL")
    model_name: str = Field(default="llama3", description="Ollama model adÄ±")
    
    def __init__(self, base_url: str, model_name: str = "llama3", **kwargs):
        # URL'yi temizle
        base_url = base_url.rstrip('/')
        super().__init__(base_url=base_url, model_name=model_name, **kwargs)
        
    def _generate(self, messages, stop=None, run_manager=None, **kwargs):
        """LangChain'in beklediÄŸi format iÃ§in messages'Ä± dÃ¶nÃ¼ÅŸtÃ¼r."""
        api_url = f"{self.base_url}/api/chat"
        
        # LangChain messages formatÄ±nÄ± Ollama formatÄ±na Ã§evir
        ollama_messages = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                ollama_messages.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                ollama_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                ollama_messages.append({"role": "assistant", "content": msg.content})
        
        payload = {
            "model": self.model_name,
            "messages": ollama_messages,
            "stream": False
        }
        
        try:
            print(f"ğŸ”· Ollama (Llama) modeline istek gÃ¶nderiliyor: {self.model_name}...")
            response = requests.post(api_url, json=payload, timeout=300)
            response.raise_for_status()
            
            data = response.json()
            content = data["message"]["content"]
            
            # Usage metadata'yÄ± sakla (token tracking iÃ§in)
            usage_info = {
                "prompt_eval_count": data.get("prompt_eval_count", 0),
                "eval_count": data.get("eval_count", 0)
            }
            
            # LangChain uyumlu AIMessage oluÅŸtur
            ai_message = AIMessage(content=content)
            
            # Usage metadata'yÄ± response_metadata'ya ekle
            ai_message.response_metadata = {
                "usage_metadata": usage_info,
                "model": self.model_name,
                "base_url": self.base_url
            }
            
            # LangChain'in beklediÄŸi ChatResult formatÄ±nÄ± dÃ¶ndÃ¼r
            generation = ChatGeneration(message=ai_message)
            return ChatResult(generations=[generation])
        except requests.exceptions.RequestException as e:
            print(f"âŒ Ollama API hatasÄ±: {e}")
            raise Exception(f"Ollama (Llama) modeline eriÅŸilemedi: {e}") from e
    
    @property
    def _llm_type(self) -> str:
        return "ollama"


def get_llm_for_model(model_name: str) -> BaseChatModel:
    """Model adÄ±na gÃ¶re uygun LLM'i dÃ¶ndÃ¼rÃ¼r."""
    print(f"ğŸ¤– Model seÃ§iliyor: {model_name}")
    
    if model_name == "gemini":
        try:
            llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", temperature=0, google_api_key=GEMINI_API_KEY)
            print("âœ… Gemini modeli yÃ¼klendi")
            return llm
        except Exception as e:
            print(f"âŒ Gemini modeli yÃ¼klenemedi: {e}")
            raise Exception(f"Gemini modeli yÃ¼klenemedi: {e}") from e
    
    elif model_name == "gpt-4o":
        try:
            if not OPENAI_API_KEY:
                raise Exception("OPENAI_API_KEY yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")
            llm = ChatOpenAI(model="gpt-4o", temperature=0, api_key=OPENAI_API_KEY)
            print("âœ… GPT-4o modeli yÃ¼klendi")
            return llm
        except Exception as e:
            print(f"âŒ GPT-4o modeli yÃ¼klenemedi: {e}")
            raise Exception(f"GPT-4o modeli yÃ¼klenemedi: {e}") from e
    
    elif model_name == "claude":
        try:
            if not ANTHROPIC_API_KEY:
                raise Exception("ANTHROPIC_API_KEY yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")
            llm = ChatAnthropic(model="claude-3-haiku-20240307", temperature=0, api_key=ANTHROPIC_API_KEY)
            print("âœ… Claude modeli yÃ¼klendi")
            return llm
        except Exception as e:
            print(f"âŒ Claude modeli yÃ¼klenemedi: {e}")
            raise Exception(f"Claude modeli yÃ¼klenemedi: {e}") from e
    
    elif model_name == "llama":
        try:
            if not OLLAMA_API_BASE_URL:
                raise Exception(
                    "OLLAMA_API_BASE_URL yapÄ±landÄ±rÄ±lmamÄ±ÅŸ. "
                    "LÃ¼tfen environment variable'Ä± ayarlayÄ±n veya Ollama'nÄ±n http://localhost:11434 adresinde Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun. "
                    "Ollama'yÄ± baÅŸlatmak iÃ§in terminal'de 'ollama serve' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."
                )
            
            print(f"ğŸ”— Ollama baÄŸlantÄ±sÄ± deneniyor: {OLLAMA_API_BASE_URL}")
            llm = OllamaLLMWrapper(base_url=OLLAMA_API_BASE_URL, model_name="llama3")
            print("âœ… Llama (Ollama) modeli yÃ¼klendi")
            return llm
        except requests.exceptions.ConnectionError as e:
            error_msg = (
                f"Ollama sunucusuna baÄŸlanÄ±lamadÄ±. LÃ¼tfen Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.\n"
                f"  - Ollama URL: {OLLAMA_API_BASE_URL}\n"
                f"  - Ollama'yÄ± baÅŸlatmak iÃ§in: 'ollama serve' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n\n"
                f"  - FarklÄ± bir URL kullanmak iÃ§in: OLLAMA_API_BASE_URL environment variable'Ä±nÄ± ayarlayÄ±n"
            )
            print(f"âŒ {error_msg}")
            raise Exception(error_msg) from e
        except Exception as e:
            error_msg = f"Llama modeli yÃ¼klenemedi: {e}"
            print(f"âŒ {error_msg}")
            raise Exception(error_msg) from e
    
    else:
        raise Exception(f"Bilinmeyen model: {model_name}")


def get_cheap_llm() -> BaseChatModel:
    """
    Daha dÃ¼ÅŸÃ¼k maliyetli iÅŸlemler (Reranking, HyDE vb.) iÃ§in ucuz bir model dÃ¶ndÃ¼rÃ¼r.
    VarsayÄ±lan olarak Gemini Flash kullanÄ±lÄ±r.
    """
    try:
        # Gemini Flash ÅŸu an en iyi F/P oranÄ±na sahip
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", temperature=0, google_api_key=GEMINI_API_KEY)
        # print("âœ… Ucuz model (Gemini Flash) yÃ¼klendi")
        return llm
    except Exception as e:
        print(f"âš ï¸ Ucuz model yÃ¼klenemedi, fallback olarak GPT-3.5 veya mevcut diÄŸer modeller denenebilir: {e}")
        # Fallback mekanizmasÄ± eklenebilir
        raise e


